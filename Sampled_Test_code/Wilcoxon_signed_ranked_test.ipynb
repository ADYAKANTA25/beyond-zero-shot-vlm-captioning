{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UvvK6U7vGYOJ"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from scipy import stats\n",
        "# import numpy as np\n",
        "\n",
        "# # 1. Load the Two Generated CSV Files\n",
        "# # 1. Load the Data\n",
        "# file_zs = 'qwen2_ft_zs_full_metrics.csv'\n",
        "# file_fs = 'qwen2_ft_fs_full_metrics.csv'\n",
        "\n",
        "# df_zs = pd.read_csv(file_zs)\n",
        "# df_fs = pd.read_csv(file_fs)\n",
        "\n",
        "# # 2. Ensure Images align (Sort by Image ID just in case)\n",
        "# df_zs = df_zs.sort_values('image_id').reset_index(drop=True)\n",
        "# df_fs = df_fs.sort_values('image_id').reset_index(drop=True)\n",
        "\n",
        "# # 3. Define Metrics List\n",
        "# metrics = [\n",
        "#     'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4',\n",
        "#     'ROUGE-1', 'ROUGE-2', 'ROUGE-L',\n",
        "#     'METEOR', 'CIDEr',\n",
        "#     'BERTScore', 'CLIPScore', 'RefCLIPScore',\n",
        "#     'Distinct-1', 'Distinct-2'\n",
        "# ]\n",
        "\n",
        "# # 4. Perform Wilcoxon Test\n",
        "# print(f\"{'Metric':<15} | {'P-Value':<12} | {'Verdict'}\")\n",
        "# print(\"-\" * 50)\n",
        "\n",
        "# for metric in metrics:\n",
        "#     # Get the two lists of scores\n",
        "#     scores_zs = df_zs[metric]\n",
        "#     scores_fs = df_fs[metric]\n",
        "\n",
        "#     # Perform Wilcoxon Signed-Rank Test\n",
        "#     # 'two-sided' checks for ANY significant difference\n",
        "#     stat, p_value = stats.wilcoxon(scores_zs, scores_fs)\n",
        "\n",
        "#     # Determine Significance\n",
        "#     if p_value < 0.05:\n",
        "#         # Check Direction (Did it improve or degrade?)\n",
        "#         mean_diff = scores_fs.mean() - scores_zs.mean()\n",
        "#         direction = \"Improved (+)\" if mean_diff > 0 else \"Degraded (-)\"\n",
        "#         verdict = f\"✅ Sig ({direction})\"\n",
        "#     else:\n",
        "#         verdict = \"❌ Not Sig\"\n",
        "\n",
        "#     print(f\"{metric:<15} | {p_value:.2e}     | {verdict}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from scipy import stats\n",
        "\n",
        "# # 1. Load the Data\n",
        "# file_zs = 'qwen2_ft_zs_full_metrics.csv'\n",
        "# file_fs = 'qwen2_ft_fs_full_metrics.csv'\n",
        "\n",
        "# df_zs = pd.read_csv(file_zs)\n",
        "# df_fs = pd.read_csv(file_fs)\n",
        "\n",
        "# # Ensure alignment\n",
        "# df_zs = df_zs.sort_values('image_id').reset_index(drop=True)\n",
        "# df_fs = df_fs.sort_values('image_id').reset_index(drop=True)\n",
        "\n",
        "# metrics = [\n",
        "#     'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4',\n",
        "#     'ROUGE-1', 'ROUGE-2', 'ROUGE-L',\n",
        "#     'METEOR', 'CIDEr',\n",
        "#     'BERTScore', 'CLIPScore', 'RefCLIPScore',\n",
        "#     'Distinct-1', 'Distinct-2'\n",
        "# ]\n",
        "\n",
        "# # 2. Bootstrap Function for 95% CI\n",
        "# def get_bootstrap_ci(data, n_bootstrap=1000, ci=0.95):\n",
        "#     \"\"\"Calculates the 95% Confidence Interval for the mean.\"\"\"\n",
        "#     means = []\n",
        "#     n = len(data)\n",
        "#     for _ in range(n_bootstrap):\n",
        "#         # Sample with replacement\n",
        "#         sample = np.random.choice(data, size=n, replace=True)\n",
        "#         means.append(np.mean(sample))\n",
        "\n",
        "#     lower = np.percentile(means, (1 - ci) / 2 * 100)\n",
        "#     upper = np.percentile(means, (1 + ci) / 2 * 100)\n",
        "#     return lower, upper\n",
        "\n",
        "# # 3. Main Loop\n",
        "# p_value_data = []\n",
        "# ci_data = []\n",
        "\n",
        "# for metric in metrics:\n",
        "#     scores_zs = df_zs[metric].values\n",
        "#     scores_fs = df_fs[metric].values\n",
        "\n",
        "#     # --- A. P-Values (Wilcoxon) ---\n",
        "#     stat, p_val = stats.wilcoxon(scores_zs, scores_fs)\n",
        "\n",
        "#     # Verdict Logic\n",
        "#     if p_val < 0.05:\n",
        "#         verdict = \"Significant\"\n",
        "#         direction = \"Improved (+)\" if np.mean(scores_fs) > np.mean(scores_zs) else \"Degraded (-)\"\n",
        "#     else:\n",
        "#         verdict = \"Not Significant\"\n",
        "#         direction = \"-\"\n",
        "\n",
        "#     p_value_data.append({\n",
        "#         'Metric': metric,\n",
        "#         'P_Value': p_val,\n",
        "#         'Significance': verdict,\n",
        "#         'Direction': direction\n",
        "#     })\n",
        "\n",
        "#     # --- B. Confidence Intervals (Bootstrapping) ---\n",
        "#     zs_mean = np.mean(scores_zs)\n",
        "#     fs_mean = np.mean(scores_fs)\n",
        "\n",
        "#     zs_ci_low, zs_ci_high = get_bootstrap_ci(scores_zs)\n",
        "#     fs_ci_low, fs_ci_high = get_bootstrap_ci(scores_fs)\n",
        "\n",
        "#     ci_data.append({\n",
        "#         'Metric': metric,\n",
        "#         'ZS_Mean': zs_mean,\n",
        "#         'ZS_CI_Lower': zs_ci_low,\n",
        "#         'ZS_CI_Upper': zs_ci_high,\n",
        "#         'FS_Mean': fs_mean,\n",
        "#         'FS_CI_Lower': fs_ci_low,\n",
        "#         'FS_CI_Upper': fs_ci_high\n",
        "#     })\n",
        "\n",
        "# # 4. Save to 2 Separate CSV Files\n",
        "# df_p_values = pd.DataFrame(p_value_data)\n",
        "# df_cis = pd.DataFrame(ci_data)\n",
        "\n",
        "# df_p_values.to_csv('P_Values_qwen2_ft(zs)_vs_ft(fs).csv', index=False)\n",
        "# df_cis.to_csv('CIs_qwen2_ft(zs)_vs_ft(fs).csv', index=False)\n",
        "\n",
        "# print(\"File 1: Wilcoxon_P_Values.csv\")\n",
        "# print(df_p_values)\n",
        "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "# print(\"File 2: Confidence_Intervals.csv\")\n",
        "# print(df_cis)"
      ],
      "metadata": {
        "id": "5N-L36KQHmPb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEW-FORMAT**"
      ],
      "metadata": {
        "id": "M_AsrQWUd1IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CIs Scores**"
      ],
      "metadata": {
        "id": "LYHOluWommCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# INPUT: Set your file path here\n",
        "# ==========================================\n",
        "input_file = \"LLama_FT_zs_full_metrics.csv\"\n",
        "output_file = \"Llama_Ft_zs_CIs_Individual.csv\"\n",
        "# ==========================================\n",
        "\n",
        "metrics = [\n",
        "    'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4',\n",
        "    'ROUGE-1', 'ROUGE-2', 'ROUGE-L',\n",
        "    'METEOR', 'CIDEr',\n",
        "    'BERTScore', 'CLIPScore', 'RefCLIPScore',\n",
        "    'Distinct-1', 'Distinct-2'\n",
        "]\n",
        "\n",
        "def get_bootstrap_ci(data, n_bootstrap=10000, ci=0.95):\n",
        "    \"\"\"\n",
        "    Calculates Mean and 95% CI using Bootstrapping.\n",
        "    n_bootstrap=10000 is standard for high-impact journals.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    # 1. True Mean (matches your Master Table)\n",
        "    mean_val = np.mean(data)\n",
        "\n",
        "    # 2. Bootstrap\n",
        "    means = []\n",
        "    n = len(data)\n",
        "    for _ in range(n_bootstrap):\n",
        "        # Sample with replacement\n",
        "        sample = np.random.choice(data, size=n, replace=True)\n",
        "        means.append(np.mean(sample))\n",
        "\n",
        "    lower = np.percentile(means, (1 - ci) / 2 * 100)\n",
        "    upper = np.percentile(means, (1 + ci) / 2 * 100)\n",
        "\n",
        "    return mean_val, lower, upper\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv(input_file)\n",
        "print(f\"Loaded {len(df)} rows from {input_file}\")\n",
        "\n",
        "results = []\n",
        "print(f\"{'Metric':<15} | {'Mean':<10} | {'95% CI'}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "for metric in metrics:\n",
        "    if metric in df.columns:\n",
        "        scores = df[metric].dropna().values\n",
        "        mean_val, low, high = get_bootstrap_ci(scores)\n",
        "\n",
        "        # Formatting\n",
        "        ci_str = f\"[{low:.3f}, {high:.3f}]\"\n",
        "        print(f\"{metric:<15} | {mean_val:<10.3f} | {ci_str}\")\n",
        "\n",
        "        results.append({\n",
        "            'Metric': metric,\n",
        "            'Mean': mean_val,\n",
        "            'CI_Lower': low,\n",
        "            'CI_Upper': high,\n",
        "            'Formatted_CI': ci_str\n",
        "        })\n",
        "\n",
        "# Save\n",
        "pd.DataFrame(results).to_csv(output_file, index=False)\n",
        "print(f\"\\n[Done] Saved to {output_file}\")"
      ],
      "metadata": {
        "id": "CKzfkKR4j4ZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d3afa6-4c01-4dfb-ab46-17c6028597e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 rows from LLama_FT_zs_full_metrics.csv\n",
            "Metric          | Mean       | 95% CI\n",
            "---------------------------------------------\n",
            "BLEU-1          | 0.855      | [0.831, 0.879]\n",
            "BLEU-2          | 0.685      | [0.652, 0.718]\n",
            "BLEU-3          | 0.550      | [0.511, 0.588]\n",
            "BLEU-4          | 0.427      | [0.384, 0.470]\n",
            "ROUGE-1         | 0.667      | [0.643, 0.692]\n",
            "ROUGE-2         | 0.451      | [0.417, 0.487]\n",
            "ROUGE-L         | 0.636      | [0.608, 0.664]\n",
            "METEOR          | 0.637      | [0.607, 0.667]\n",
            "CIDEr           | 1.147      | [1.004, 1.299]\n",
            "BERTScore       | 0.706      | [0.685, 0.727]\n",
            "CLIPScore       | 33.728     | [33.158, 34.294]\n",
            "RefCLIPScore    | 87.902     | [86.511, 89.206]\n",
            "Distinct-1      | 0.379      | [0.366, 0.392]\n",
            "Distinct-2      | 0.621      | [0.600, 0.642]\n",
            "\n",
            "[Done] Saved to Llama_Ft_zs_CIs_Individual.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **P-TEST SCORES**"
      ],
      "metadata": {
        "id": "Wfli2YNvmpfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# ==========================================\n",
        "# INPUTS: Set your comparison files here\n",
        "# ==========================================\n",
        "file_baseline = \"qwen3_base_zs_full_metrics.csv\"  # Baseline (e.g., Zero-Shot)\n",
        "file_target   = \"qwen3_base_fs_full_metrics.csv\"  # Target (e.g., Few-Shot)\n",
        "output_file   = \"P_Values_qwen3_base(zs)_vs_base(fs).csv\"\n",
        "# ==========================================\n",
        "\n",
        "metrics = [\n",
        "    'BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4',\n",
        "    'ROUGE-1', 'ROUGE-2', 'ROUGE-L',\n",
        "    'METEOR', 'CIDEr',\n",
        "    'BERTScore', 'CLIPScore', 'RefCLIPScore',\n",
        "    'Distinct-1', 'Distinct-2'\n",
        "]\n",
        "\n",
        "# Load Data\n",
        "df_base = pd.read_csv(file_baseline)\n",
        "df_tgt = pd.read_csv(file_target)\n",
        "\n",
        "# Merge on image_id to ensure strict pairing\n",
        "# (Comparing Image 1 from Model A vs Image 1 from Model B)\n",
        "merged = pd.merge(df_base, df_tgt, on='image_id', suffixes=('_B', '_T'))\n",
        "print(f\"Aligned {len(merged)} images for comparison.\")\n",
        "\n",
        "results = []\n",
        "print(f\"{'Metric':<15} | {'P-Value':<15} | {'Verdict'}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for metric in metrics:\n",
        "    col_b = f\"{metric}_B\"\n",
        "    col_t = f\"{metric}_T\"\n",
        "\n",
        "    if col_b in merged.columns and col_t in merged.columns:\n",
        "        scores_base = merged[col_b].values\n",
        "        scores_tgt = merged[col_t].values\n",
        "\n",
        "        # 1. Wilcoxon Signed-Rank Test\n",
        "        try:\n",
        "            stat, p_val = stats.wilcoxon(scores_base, scores_tgt)\n",
        "        except ValueError:\n",
        "            p_val = 1.0 # Happens if all scores are exactly identical\n",
        "\n",
        "        # 2. Determine Direction\n",
        "        mean_diff = scores_tgt.mean() - scores_base.mean()\n",
        "        direction = \"(+)\" if mean_diff > 0 else \"(-)\"\n",
        "\n",
        "        # 3. Format P-Value\n",
        "        # Use scientific notation for very small numbers (e.g., 3.4e-10)\n",
        "        if p_val < 0.001:\n",
        "            p_str = f\"{p_val:.2e}\"\n",
        "        else:\n",
        "            p_str = f\"{p_val:.3f}\"\n",
        "\n",
        "        label = \"Sig\" if p_val < 0.05 else \"Not Sig\"\n",
        "\n",
        "        print(f\"{metric:<15} | {p_str:<15} | {label} {direction}\")\n",
        "\n",
        "        results.append({\n",
        "            'Metric': metric,\n",
        "            'P_Value': p_val,\n",
        "            'Formatted_P': p_str,\n",
        "            'Significance': label,\n",
        "            'Direction': direction\n",
        "        })\n",
        "\n",
        "# Save\n",
        "pd.DataFrame(results).to_csv(output_file, index=False)\n",
        "print(f\"\\n[Done] Saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq_hlqsFjBa2",
        "outputId": "77f6dcb0-cfde-422a-e815-b1415121a949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aligned 100 images for comparison.\n",
            "Metric          | P-Value         | Verdict\n",
            "--------------------------------------------------\n",
            "BLEU-1          | 1.42e-04        | Sig (+)\n",
            "BLEU-2          | 2.40e-10        | Sig (+)\n",
            "BLEU-3          | 1.35e-09        | Sig (+)\n",
            "BLEU-4          | 3.84e-08        | Sig (+)\n",
            "ROUGE-1         | 1.34e-12        | Sig (+)\n",
            "ROUGE-2         | 3.51e-12        | Sig (+)\n",
            "ROUGE-L         | 1.07e-10        | Sig (+)\n",
            "METEOR          | 1.37e-10        | Sig (+)\n",
            "CIDEr           | 0.130           | Not Sig (+)\n",
            "BERTScore       | 1.42e-15        | Sig (+)\n",
            "CLIPScore       | 2.40e-10        | Sig (+)\n",
            "RefCLIPScore    | 0.009           | Sig (+)\n",
            "Distinct-1      | 3.90e-18        | Sig (-)\n",
            "Distinct-2      | 3.90e-18        | Sig (-)\n",
            "\n",
            "[Done] Saved to P_Values_qwen3_base(zs)_vs_base(fs).csv\n"
          ]
        }
      ]
    }
  ]
}